<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Paul Bürkner, Jonah Gabry, Aki Vehtari" />

<meta name="date" content="2020-07-14" />

<title>Approximate leave-future-out cross-validation for Bayesian time series models</title>

<script>// Hide empty <a> tag within highlighted CodeBlock for screen reader accessibility (see https://github.com/jgm/pandoc/issues/6352#issuecomment-626106786) -->
// v0.0.1
// Written by JooYoung Seo (jooyoung@psu.edu) and Atsushi Yasumoto on June 1st, 2020.

document.addEventListener('DOMContentLoaded', function() {
  const codeList = document.getElementsByClassName("sourceCode");
  for (var i = 0; i < codeList.length; i++) {
    var linkList = codeList[i].getElementsByTagName('a');
    for (var j = 0; j < linkList.length; j++) {
      if (linkList[j].innerHTML === "") {
        linkList[j].setAttribute('aria-hidden', 'true');
      }
    }
  }
});
</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Approximate leave-future-out cross-validation for Bayesian time series models</h1>
<h4 class="author">Paul Bürkner, Jonah Gabry, Aki Vehtari</h4>
<h4 class="date">2020-07-14</h4>


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#m-step-ahead-predictions"><span class="math inline">\(M\)</span>-step-ahead predictions</a></li>
<li><a href="#approximate_MSAP">Approximate <span class="math inline">\(M\)</span>-SAP using importance-sampling</a></li>
<li><a href="#autoregressive-models">Autoregressive models</a></li>
<li><a href="#case-study-annual-measurements-of-the-level-of-lake-huron">Case Study: Annual measurements of the level of Lake Huron</a></li>
<li><a href="#step-ahead-predictions-leaving-out-all-future-values">1-step-ahead predictions leaving out all future values</a><ul>
<li><a href="#exact-1-step-ahead-predictions">Exact 1-step-ahead predictions</a></li>
<li><a href="#approximate-1-step-ahead-predictions">Approximate 1-step-ahead predictions</a></li>
</ul></li>
<li><a href="#m-step-ahead-predictions-leaving-out-all-future-values"><span class="math inline">\(M\)</span>-step-ahead predictions leaving out all future values</a><ul>
<li><a href="#exact-m-step-ahead-predictions">Exact <span class="math inline">\(M\)</span>-step-ahead predictions</a></li>
<li><a href="#approximate-m-step-ahead-predictions">Approximate <span class="math inline">\(M\)</span>-step-ahead predictions</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
<li><a href="#appendix">Appendix</a><ul>
<li><a href="#appendix-session-information">Appendix: Session information</a></li>
<li><a href="#appendix-licenses">Appendix: Licenses</a></li>
</ul></li>
</ul>
</div>

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Approximate leave-future-out cross-validation for Bayesian time series models}
-->
<p><strong>NOTE: We recommend viewing the fully rendered version of this vignette online at <a href="https://mc-stan.org/loo/articles/" class="uri">https://mc-stan.org/loo/articles/</a></strong></p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>One of the most common goals of a time series analysis is to use the observed series to inform predictions for future observations. We will refer to this task of predicting a sequence of <span class="math inline">\(M\)</span> future observations as <span class="math inline">\(M\)</span>-step-ahead prediction (<span class="math inline">\(M\)</span>-SAP). Fortunately, once we have fit a model and can sample from the posterior predictive distribution, it is straightforward to generate predictions as far into the future as we want. It is also straightforward to evaluate the <span class="math inline">\(M\)</span>-SAP performance of a time series model by comparing the predictions to the observed sequence of <span class="math inline">\(M\)</span> future data points once they become available.</p>
<p>Unfortunately, we are often in the position of having to use a model to inform decisions <em>before</em> we can collect the future observations required for assessing the predictive performance. If we have many competing models we may also need to first decide which of the models (or which combination of the models) we should rely on for predictions. In these situations the best we can do is to use methods for approximating the expected predictive performance of our models using only the observations of the time series we already have.</p>
<p>If there were no time dependence in the data or if the focus is to assess the non-time-dependent part of the model, we could use methods like leave-one-out cross-validation (LOO-CV). For a data set with <span class="math inline">\(N\)</span> observations, we refit the model <span class="math inline">\(N\)</span> times, each time leaving out one of the <span class="math inline">\(N\)</span> observations and assessing how well the model predicts the left-out observation. LOO-CV is very expensive computationally in most realistic settings, but the Pareto smoothed importance sampling (PSIS, Vehtari et al, 2017, 2019) algorithm provided by the <em>loo</em> package allows for approximating exact LOO-CV with PSIS-LOO-CV. PSIS-LOO-CV requires only a single fit of the full model and comes with diagnostics for assessing the validity of the approximation.</p>
<p>With a time series we can do something similar to LOO-CV but, except in a few cases, it does not make sense to leave out observations one at a time because then we are allowing information from the future to influence predictions of the past (i.e., times <span class="math inline">\(t + 1, t+2, \ldots\)</span> should not be used to predict for time <span class="math inline">\(t\)</span>). To apply the idea of cross-validation to the <span class="math inline">\(M\)</span>-SAP case, instead of leave-<em>one</em>-out cross-validation we need some form of leave-<em>future</em>-out cross-validation (LFO-CV). As we will demonstrate in this case study, LFO-CV does not refer to one particular prediction task but rather to various possible cross-validation approaches that all involve some form of prediction for new time series data. Like exact LOO-CV, exact LFO-CV requires refitting the model many times to different subsets of the data, which is computationally very costly for most nontrivial examples, in particular for Bayesian analyses where refitting the model means estimating a new posterior distribution rather than a point estimate.</p>
<p>Although PSIS-LOO-CV provides an efficient approximation to exact LOO-CV, until now there has not been an analogous approximation to exact LFO-CV that drastically reduces the computational burden while also providing informative diagnostics about the quality of the approximation. In this case study we present PSIS-LFO-CV, an algorithm that typically only requires refitting the time-series model a small number times and will make LFO-CV tractable for many more realistic applications than previously possible.</p>
<p>More details can be found in our paper about approximate LFO-CV (Bürkner, Gabry, &amp; Vehtari, 2020), which is available as a preprint on arXiv (<a href="https://arxiv.org/abs/1902.06281" class="uri">https://arxiv.org/abs/1902.06281</a>).</p>
</div>
<div id="m-step-ahead-predictions" class="section level2">
<h2><span class="math inline">\(M\)</span>-step-ahead predictions</h2>
<p>Assume we have a time series of observations <span class="math inline">\(y = (y_1, y_2, \ldots, y_N)\)</span> and let <span class="math inline">\(L\)</span> be the <em>minimum</em> number of observations from the series that we will require before making predictions for future data. Depending on the application and how informative the data is, it may not be possible to make reasonable predictions for <span class="math inline">\(y_{i+1}\)</span> based on <span class="math inline">\((y_1, \dots, y_{i})\)</span> until <span class="math inline">\(i\)</span> is large enough so that we can learn enough about the time series to predict future observations. Setting <span class="math inline">\(L=10\)</span>, for example, means that we will only assess predictive performance starting with observation <span class="math inline">\(y_{11}\)</span>, so that we always have at least 10 previous observations to condition on.</p>
<p>In order to assess <span class="math inline">\(M\)</span>-SAP performance we would like to compute the predictive densities</p>
<p><span class="math display">\[
p(y_{i+1:M} \,|\, y_{1:i}) = 
  p(y_{i+1}, \ldots, y_{i + M} \,|\, y_{1},...,y_{i}) 
\]</span></p>
<p>for each <span class="math inline">\(i \in \{L, \ldots, N - M\}\)</span>. The quantities <span class="math inline">\(p(y_{i+1:M} \,|\, y_{1:i})\)</span> can be computed with the help of the posterior distribution <span class="math inline">\(p(\theta \,|\, y_{1:i})\)</span> of the parameters <span class="math inline">\(\theta\)</span> conditional on only the first <span class="math inline">\(i\)</span> observations of the time-series:</p>
<p><span class="math display">\[
p(y_{i+1:M} \,| \, y_{1:i}) = 
  \int p(y_{i+1:M} \,| \, y_{1:i}, \theta) \, p(\theta\,|\,y_{1:i}) \,d\theta. 
\]</span></p>
<p>Having obtained <span class="math inline">\(S\)</span> draws <span class="math inline">\((\theta_{1:i}^{(1)}, \ldots, \theta_{1:i}^{(S)})\)</span> from the posterior distribution <span class="math inline">\(p(\theta\,|\,y_{1:i})\)</span>, we can estimate <span class="math inline">\(p(y_{i+1:M} | y_{1:i})\)</span> as</p>
<p><span class="math display">\[
p(y_{i+1:M} \,|\, y_{1:i}) \approx \frac{1}{S}\sum_{s=1}^S p(y_{i+1:M} \,|\, y_{1:i}, \theta_{1:i}^{(s)}).
\]</span></p>
</div>
<div id="approximate_MSAP" class="section level2">
<h2>Approximate <span class="math inline">\(M\)</span>-SAP using importance-sampling</h2>
<p>Unfortunately, the math above makes use of the posterior distributions from many different fits of the model to different subsets of the data. That is, to obtain the predictive density <span class="math inline">\(p(y_{i+1:M} \,|\, y_{1:i})\)</span> requires fitting a model to only the first <span class="math inline">\(i\)</span> data points, and we will need to do this for every value of <span class="math inline">\(i\)</span> under consideration (all <span class="math inline">\(i \in \{L, \ldots, N - M\}\)</span>).</p>
<p>To reduce the number of models that need to be fit for the purpose of obtaining each of the densities <span class="math inline">\(p(y_{i+1:M} \,|\, y_{1:i})\)</span>, we propose the following algorithm. First, we refit the model using the first <span class="math inline">\(L\)</span> observations of the time series and then perform a single exact <span class="math inline">\(M\)</span>-step-ahead prediction step for <span class="math inline">\(p(y_{L+1:M} \,|\, y_{1:L})\)</span>. Recall that <span class="math inline">\(L\)</span> is the minimum number of observations we have deemed acceptable for making predictions (setting <span class="math inline">\(L=0\)</span> means the first data point will be predicted only based on the prior). We define <span class="math inline">\(i^\star = L\)</span> as the current point of refit. Next, starting with <span class="math inline">\(i = i^\star + 1\)</span>, we approximate each <span class="math inline">\(p(y_{i+1:M} \,|\, y_{1:i})\)</span> via</p>
<p><span class="math display">\[
 p(y_{i+1:M} \,|\, y_{1:i}) \approx
   \frac{ \sum_{s=1}^S w_i^{(s)}\, p(y_{i+1:M} \,|\, y_{1:i}, \theta^{(s)})}
        { \sum_{s=1}^S w_i^{(s)}},
\]</span></p>
<p>where <span class="math inline">\(\theta^{(s)} = \theta^{(s)}_{1:i^\star}\)</span> are draws from the posterior distribution based on the first <span class="math inline">\(i^\star\)</span> observations and <span class="math inline">\(w_i^{(s)}\)</span> are the PSIS weights obtained in two steps. First, we compute the raw importance ratios</p>
<p><span class="math display">\[
r_i^{(s)} =
\frac{f_{1:i}(\theta^{(s)})}{f_{1:i^\star}(\theta^{(s)})} 
\propto \prod_{j \in (i^\star + 1):i} p(y_j \,|\, y_{1:(j-1)}, \theta^{(s)}),
\]</span></p>
<p>and then stabilize them using PSIS. The function <span class="math inline">\(f_{1:i}\)</span> denotes the posterior distribution based on the first <span class="math inline">\(i\)</span> observations, that is, <span class="math inline">\(f_{1:i} = p(\theta \,|\, y_{1:i})\)</span>, with <span class="math inline">\(f_{1:i^\star}\)</span> defined analogously. The index set <span class="math inline">\((i^\star + 1):i\)</span> indicates all observations which are part of the data for the model <span class="math inline">\(f_{1:i}\)</span> whose predictive performance we are trying to approximate but not for the actually fitted model <span class="math inline">\(f_{1:i^\star}\)</span>. The proportional statement arises from the fact that we ignore the normalizing constants <span class="math inline">\(p(y_{1:i})\)</span> and <span class="math inline">\(p(y_{1:i^\star})\)</span> of the compared posteriors, which leads to a self-normalized variant of PSIS (see Vehtari et al, 2017).</p>
<p>Continuing with the next observation, we gradually increase <span class="math inline">\(i\)</span> by <span class="math inline">\(1\)</span> (we move forward in time) and repeat the process. At some observation <span class="math inline">\(i\)</span>, the variability of the importance ratios <span class="math inline">\(r_i^{(s)}\)</span> will become too large and importance sampling will fail. We will refer to this particular value of <span class="math inline">\(i\)</span> as <span class="math inline">\(i^\star_1\)</span>. To identify the value of <span class="math inline">\(i^\star_1\)</span>, we check for which value of <span class="math inline">\(i\)</span> does the estimated shape parameter <span class="math inline">\(k\)</span> of the generalized Pareto distribution first cross a certain threshold <span class="math inline">\(\tau\)</span> (Vehtari et al, 2019). Only then do we refit the model using the observations up to <span class="math inline">\(i^\star_1\)</span> and restart the process from there by setting <span class="math inline">\(\theta^{(s)} = \theta^{(s)}_{1:i^\star_1}\)</span> and <span class="math inline">\(i^\star = i^\star_1\)</span> until the next refit.</p>
<p>In some cases we may only need to refit once and in other cases we will find a value <span class="math inline">\(i^\star_2\)</span> that requires a second refitting, maybe an <span class="math inline">\(i^\star_3\)</span> that requires a third refitting, and so on. We refit as many times as is required (only when <span class="math inline">\(k &gt; \tau\)</span>) until we arrive at observation <span class="math inline">\(i = N - M\)</span>. For LOO, we recommend to use a threshold of <span class="math inline">\(\tau = 0.7\)</span> (Vehtari et al, 2017, 2019) and it turns out this is a reasonable threshold for LFO as well (Bürkner et al. 2020).</p>
</div>
<div id="autoregressive-models" class="section level2">
<h2>Autoregressive models</h2>
<p>Autoregressive (AR) models are some of the most commonly used time-series models. An AR(p) model —an autoregressive model of order <span class="math inline">\(p\)</span>— can be defined as</p>
<p><span class="math display">\[
y_i = \eta_i + \sum_{k = 1}^p \varphi_k y_{i - k} + \varepsilon_i,
\]</span></p>
<p>where <span class="math inline">\(\eta_i\)</span> is the linear predictor for the <span class="math inline">\(i\)</span>th observation, <span class="math inline">\(\phi_k\)</span> are the autoregressive parameters and <span class="math inline">\(\varepsilon_i\)</span> are pairwise independent errors, which are usually assumed to be normally distributed with equal variance <span class="math inline">\(\sigma^2\)</span>. The model implies a recursive formula that allows for computing the right-hand side of the above equation for observation <span class="math inline">\(i\)</span> based on the values of the equations for previous observations.</p>
</div>
<div id="case-study-annual-measurements-of-the-level-of-lake-huron" class="section level2">
<h2>Case Study: Annual measurements of the level of Lake Huron</h2>
<p>To illustrate the application of PSIS-LFO-CV for estimating expected <span class="math inline">\(M\)</span>-SAP performance, we will fit a model for 98 annual measurements of the water level (in feet) of <a href="https://en.wikipedia.org/wiki/Lake_Huron">Lake Huron</a> from the years 1875–1972. This data set is found in the <strong>datasets</strong> R package, which is installed automatically with <strong>R</strong>.</p>
<p>In addition to the <strong>loo</strong> package, for this analysis we will use the <strong>brms</strong> interface to Stan to generate a Stan program and fit the model, and also the <strong>bayesplot</strong> and <strong>ggplot2</strong> packages for plotting.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">library</span>(<span class="st">&quot;loo&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="kw">library</span>(<span class="st">&quot;brms&quot;</span>)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="kw">library</span>(<span class="st">&quot;bayesplot&quot;</span>)</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">library</span>(<span class="st">&quot;ggplot2&quot;</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="kw">color_scheme_set</span>(<span class="st">&quot;brightblue&quot;</span>)</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="kw">theme_set</span>(<span class="kw">theme_default</span>())</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a>CHAINS &lt;-<span class="st"> </span><span class="dv">4</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>SEED &lt;-<span class="st"> </span><span class="dv">5838296</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="kw">set.seed</span>(SEED)</span></code></pre></div>
<p>Before fitting a model, we will first put the data into a data frame and then look at the time series.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>N &lt;-<span class="st"> </span><span class="kw">length</span>(LakeHuron)</span>
<span id="cb2-2"><a href="#cb2-2"></a>df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb2-3"><a href="#cb2-3"></a>  <span class="dt">y =</span> <span class="kw">as.numeric</span>(LakeHuron),</span>
<span id="cb2-4"><a href="#cb2-4"></a>  <span class="dt">year =</span> <span class="kw">as.numeric</span>(<span class="kw">time</span>(LakeHuron)),</span>
<span id="cb2-5"><a href="#cb2-5"></a>  <span class="dt">time =</span> <span class="dv">1</span><span class="op">:</span>N</span>
<span id="cb2-6"><a href="#cb2-6"></a>)</span>
<span id="cb2-7"><a href="#cb2-7"></a></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="kw">ggplot</span>(df, <span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="st">  </span><span class="kw">labs</span>(</span>
<span id="cb2-11"><a href="#cb2-11"></a>    <span class="dt">y =</span> <span class="st">&quot;Water Level (ft)&quot;</span>, </span>
<span id="cb2-12"><a href="#cb2-12"></a>    <span class="dt">x =</span> <span class="st">&quot;Year&quot;</span>,</span>
<span id="cb2-13"><a href="#cb2-13"></a>    <span class="dt">title =</span> <span class="st">&quot;Water Level in Lake Huron (1875-1972)&quot;</span></span>
<span id="cb2-14"><a href="#cb2-14"></a>  ) </span></code></pre></div>
<p>The above plot shows rather strong autocorrelation of the time-series as well as some trend towards lower levels for later points in time.</p>
<p>We can specify an AR(4) model for these data using the <strong>brms</strong> package as follows:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>fit &lt;-<span class="st"> </span><span class="kw">brm</span>(</span>
<span id="cb3-2"><a href="#cb3-2"></a>  y <span class="op">~</span><span class="st"> </span><span class="kw">ar</span>(time, <span class="dt">p =</span> <span class="dv">4</span>), </span>
<span id="cb3-3"><a href="#cb3-3"></a>  <span class="dt">data =</span> df, </span>
<span id="cb3-4"><a href="#cb3-4"></a>  <span class="dt">prior =</span> <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> <span class="st">&quot;ar&quot;</span>),</span>
<span id="cb3-5"><a href="#cb3-5"></a>  <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.99</span>), </span>
<span id="cb3-6"><a href="#cb3-6"></a>  <span class="dt">seed =</span> SEED, </span>
<span id="cb3-7"><a href="#cb3-7"></a>  <span class="dt">chains =</span> CHAINS</span>
<span id="cb3-8"><a href="#cb3-8"></a>)</span></code></pre></div>
<p>The model implied predictions along with the observed values can be plotted, which reveals a rather good fit to the data.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>preds &lt;-<span class="st"> </span><span class="kw">posterior_predict</span>(fit)</span>
<span id="cb4-2"><a href="#cb4-2"></a>preds &lt;-<span class="st"> </span><span class="kw">cbind</span>(</span>
<span id="cb4-3"><a href="#cb4-3"></a>  <span class="dt">Estimate =</span> <span class="kw">colMeans</span>(preds), </span>
<span id="cb4-4"><a href="#cb4-4"></a>  <span class="dt">Q5 =</span> <span class="kw">apply</span>(preds, <span class="dv">2</span>, quantile, <span class="dt">probs =</span> <span class="fl">0.05</span>),</span>
<span id="cb4-5"><a href="#cb4-5"></a>  <span class="dt">Q95 =</span> <span class="kw">apply</span>(preds, <span class="dv">2</span>, quantile, <span class="dt">probs =</span> <span class="fl">0.95</span>)</span>
<span id="cb4-6"><a href="#cb4-6"></a>)</span>
<span id="cb4-7"><a href="#cb4-7"></a></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="kw">ggplot</span>(<span class="kw">cbind</span>(df, preds), <span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> Estimate)) <span class="op">+</span></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> Q5, <span class="dt">ymax =</span> Q95), <span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">size =</span> <span class="fl">0.5</span>) <span class="op">+</span></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="st">  </span><span class="kw">labs</span>(</span>
<span id="cb4-12"><a href="#cb4-12"></a>    <span class="dt">y =</span> <span class="st">&quot;Water Level (ft)&quot;</span>, </span>
<span id="cb4-13"><a href="#cb4-13"></a>    <span class="dt">x =</span> <span class="st">&quot;Year&quot;</span>,</span>
<span id="cb4-14"><a href="#cb4-14"></a>    <span class="dt">title =</span> <span class="st">&quot;Water Level in Lake Huron (1875-1972)&quot;</span>,</span>
<span id="cb4-15"><a href="#cb4-15"></a>    <span class="dt">subtitle =</span> <span class="st">&quot;Mean (blue) and 90% predictive intervals (gray) vs. observed data (black)&quot;</span></span>
<span id="cb4-16"><a href="#cb4-16"></a>  ) </span></code></pre></div>
<p>To allow for reasonable predictions of future values, we will require at least <span class="math inline">\(L = 20\)</span> historical observations (20 years) to make predictions.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a>L &lt;-<span class="st"> </span><span class="dv">20</span></span></code></pre></div>
<p>We first perform approximate leave-one-out cross-validation (LOO-CV) for the purpose of later comparison with exact and approximate LFO-CV for the 1-SAP case.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>loo_cv &lt;-<span class="st"> </span><span class="kw">loo</span>(<span class="kw">log_lik</span>(fit)[, (L <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>N])</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="kw">print</span>(loo_cv)</span></code></pre></div>
</div>
<div id="step-ahead-predictions-leaving-out-all-future-values" class="section level2">
<h2>1-step-ahead predictions leaving out all future values</h2>
<p>The most basic version of <span class="math inline">\(M\)</span>-SAP is 1-SAP, in which we predict only one step ahead. In this case, <span class="math inline">\(y_{i+1:M}\)</span> simplifies to <span class="math inline">\(y_{i}\)</span> and the LFO-CV algorithm becomes considerably simpler than for larger values of <span class="math inline">\(M\)</span>.</p>
<div id="exact-1-step-ahead-predictions" class="section level3">
<h3>Exact 1-step-ahead predictions</h3>
<p>Before we compute approximate LFO-CV using PSIS we will first compute exact LFO-CV for the 1-SAP case so we can use it as a benchmark later. The initial step for the exact computation is to calculate the log-predictive densities by refitting the model many times:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>loglik_exact &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> <span class="kw">nsamples</span>(fit), <span class="dt">ncol =</span> N)</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="cf">for</span> (i <span class="cf">in</span> L<span class="op">:</span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)) {</span>
<span id="cb7-3"><a href="#cb7-3"></a>  past &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>i</span>
<span id="cb7-4"><a href="#cb7-4"></a>  oos &lt;-<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb7-5"><a href="#cb7-5"></a>  df_past &lt;-<span class="st"> </span>df[past, , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb7-6"><a href="#cb7-6"></a>  df_oos &lt;-<span class="st"> </span>df[<span class="kw">c</span>(past, oos), , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb7-7"><a href="#cb7-7"></a>  fit_i &lt;-<span class="st"> </span><span class="kw">update</span>(fit, <span class="dt">newdata =</span> df_past, <span class="dt">recompile =</span> <span class="ot">FALSE</span>)</span>
<span id="cb7-8"><a href="#cb7-8"></a>  loglik_exact[, i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">log_lik</span>(fit_i, <span class="dt">newdata =</span> df_oos, <span class="dt">oos =</span> oos)[, oos]</span>
<span id="cb7-9"><a href="#cb7-9"></a>}</span></code></pre></div>
<p>Then we compute the exact expected log predictive density (ELPD):</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># some helper functions we&#39;ll use throughout</span></span>
<span id="cb8-2"><a href="#cb8-2"></a></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co"># more stable than log(sum(exp(x))) </span></span>
<span id="cb8-4"><a href="#cb8-4"></a>log_sum_exp &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb8-5"><a href="#cb8-5"></a>  max_x &lt;-<span class="st"> </span><span class="kw">max</span>(x)  </span>
<span id="cb8-6"><a href="#cb8-6"></a>  max_x <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">sum</span>(<span class="kw">exp</span>(x <span class="op">-</span><span class="st"> </span>max_x)))</span>
<span id="cb8-7"><a href="#cb8-7"></a>}</span>
<span id="cb8-8"><a href="#cb8-8"></a></span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="co"># more stable than log(mean(exp(x)))</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>log_mean_exp &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb8-11"><a href="#cb8-11"></a>  <span class="kw">log_sum_exp</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">length</span>(x))</span>
<span id="cb8-12"><a href="#cb8-12"></a>}</span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a><span class="co"># compute log of raw importance ratios</span></span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="co"># sums over observations *not* over posterior samples</span></span>
<span id="cb8-16"><a href="#cb8-16"></a>sum_log_ratios &lt;-<span class="st"> </span><span class="cf">function</span>(loglik, <span class="dt">ids =</span> <span class="ot">NULL</span>) {</span>
<span id="cb8-17"><a href="#cb8-17"></a>  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(ids)) loglik &lt;-<span class="st"> </span>loglik[, ids, drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb8-18"><a href="#cb8-18"></a>  <span class="kw">rowSums</span>(loglik)</span>
<span id="cb8-19"><a href="#cb8-19"></a>}</span>
<span id="cb8-20"><a href="#cb8-20"></a></span>
<span id="cb8-21"><a href="#cb8-21"></a><span class="co"># for printing comparisons later</span></span>
<span id="cb8-22"><a href="#cb8-22"></a>rbind_print &lt;-<span class="st"> </span><span class="cf">function</span>(...) {</span>
<span id="cb8-23"><a href="#cb8-23"></a>  <span class="kw">round</span>(<span class="kw">rbind</span>(...), <span class="dt">digits =</span> <span class="dv">2</span>)</span>
<span id="cb8-24"><a href="#cb8-24"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>exact_elpds_1sap &lt;-<span class="st"> </span><span class="kw">apply</span>(loglik_exact, <span class="dv">2</span>, log_mean_exp)</span>
<span id="cb9-2"><a href="#cb9-2"></a>exact_elpd_1sap &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">ELPD =</span> <span class="kw">sum</span>(exact_elpds_1sap[<span class="op">-</span>(<span class="dv">1</span><span class="op">:</span>L)]))</span>
<span id="cb9-3"><a href="#cb9-3"></a></span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="kw">rbind_print</span>(</span>
<span id="cb9-5"><a href="#cb9-5"></a>  <span class="st">&quot;LOO&quot;</span> =<span class="st"> </span>loo_cv<span class="op">$</span>estimates[<span class="st">&quot;elpd_loo&quot;</span>, <span class="st">&quot;Estimate&quot;</span>],</span>
<span id="cb9-6"><a href="#cb9-6"></a>  <span class="st">&quot;LFO&quot;</span> =<span class="st"> </span>exact_elpd_1sap</span>
<span id="cb9-7"><a href="#cb9-7"></a>)</span></code></pre></div>
<p>We see that the ELPD from LFO-CV for 1-step-ahead predictions is lower than the ELPD estimate from LOO-CV, which should be expected since LOO-CV is making use of more of the time series. That is, since the LFO-CV approach only uses observations from before the left-out data point but LOO-CV uses <em>all</em> data points other than the left-out observation, we should expect to see the larger ELPD from LOO-CV.</p>
</div>
<div id="approximate-1-step-ahead-predictions" class="section level3">
<h3>Approximate 1-step-ahead predictions</h3>
<p>We compute approximate 1-SAP with refit at observations where the Pareto <span class="math inline">\(k\)</span> estimate exceeds the threshold of <span class="math inline">\(0.7\)</span>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a>k_thres &lt;-<span class="st"> </span><span class="fl">0.7</span></span></code></pre></div>
<p>The code becomes a little bit more involved as compared to the exact LFO-CV. Note that we can compute exact 1-SAP at the refitting points, which comes with no additional computational costs since we had to refit the model anyway.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a>approx_elpds_1sap &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, N)</span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="co"># initialize the process for i = L</span></span>
<span id="cb11-4"><a href="#cb11-4"></a>past &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>L</span>
<span id="cb11-5"><a href="#cb11-5"></a>oos &lt;-<span class="st"> </span>L <span class="op">+</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>df_past &lt;-<span class="st"> </span>df[past, , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb11-7"><a href="#cb11-7"></a>df_oos &lt;-<span class="st"> </span>df[<span class="kw">c</span>(past, oos), , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb11-8"><a href="#cb11-8"></a>fit_past &lt;-<span class="st"> </span><span class="kw">update</span>(fit, <span class="dt">newdata =</span> df_past, <span class="dt">recompile =</span> <span class="ot">FALSE</span>)</span>
<span id="cb11-9"><a href="#cb11-9"></a>loglik &lt;-<span class="st"> </span><span class="kw">log_lik</span>(fit_past, <span class="dt">newdata =</span> df_oos, <span class="dt">oos =</span> oos)</span>
<span id="cb11-10"><a href="#cb11-10"></a>approx_elpds_1sap[L <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">log_mean_exp</span>(loglik[, oos])</span>
<span id="cb11-11"><a href="#cb11-11"></a></span>
<span id="cb11-12"><a href="#cb11-12"></a><span class="co"># iterate over i &gt; L</span></span>
<span id="cb11-13"><a href="#cb11-13"></a>i_refit &lt;-<span class="st"> </span>L</span>
<span id="cb11-14"><a href="#cb11-14"></a>refits &lt;-<span class="st"> </span>L</span>
<span id="cb11-15"><a href="#cb11-15"></a>ks &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb11-16"><a href="#cb11-16"></a><span class="cf">for</span> (i <span class="cf">in</span> (L <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)) {</span>
<span id="cb11-17"><a href="#cb11-17"></a>  past &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>i</span>
<span id="cb11-18"><a href="#cb11-18"></a>  oos &lt;-<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb11-19"><a href="#cb11-19"></a>  df_past &lt;-<span class="st"> </span>df[past, , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb11-20"><a href="#cb11-20"></a>  df_oos &lt;-<span class="st"> </span>df[<span class="kw">c</span>(past, oos), , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb11-21"><a href="#cb11-21"></a>  loglik &lt;-<span class="st"> </span><span class="kw">log_lik</span>(fit_past, <span class="dt">newdata =</span> df_oos, <span class="dt">oos =</span> oos)</span>
<span id="cb11-22"><a href="#cb11-22"></a>  </span>
<span id="cb11-23"><a href="#cb11-23"></a>  logratio &lt;-<span class="st"> </span><span class="kw">sum_log_ratios</span>(loglik, (i_refit <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>i)</span>
<span id="cb11-24"><a href="#cb11-24"></a>  psis_obj &lt;-<span class="st"> </span><span class="kw">suppressWarnings</span>(<span class="kw">psis</span>(logratio))</span>
<span id="cb11-25"><a href="#cb11-25"></a>  k &lt;-<span class="st"> </span><span class="kw">pareto_k_values</span>(psis_obj)</span>
<span id="cb11-26"><a href="#cb11-26"></a>  ks &lt;-<span class="st"> </span><span class="kw">c</span>(ks, k)</span>
<span id="cb11-27"><a href="#cb11-27"></a>  <span class="cf">if</span> (k <span class="op">&gt;</span><span class="st"> </span>k_thres) {</span>
<span id="cb11-28"><a href="#cb11-28"></a>    <span class="co"># refit the model based on the first i observations</span></span>
<span id="cb11-29"><a href="#cb11-29"></a>    i_refit &lt;-<span class="st"> </span>i</span>
<span id="cb11-30"><a href="#cb11-30"></a>    refits &lt;-<span class="st"> </span><span class="kw">c</span>(refits, i)</span>
<span id="cb11-31"><a href="#cb11-31"></a>    fit_past &lt;-<span class="st"> </span><span class="kw">update</span>(fit_past, <span class="dt">newdata =</span> df_past, <span class="dt">recompile =</span> <span class="ot">FALSE</span>)</span>
<span id="cb11-32"><a href="#cb11-32"></a>    loglik &lt;-<span class="st"> </span><span class="kw">log_lik</span>(fit_past, <span class="dt">newdata =</span> df_oos, <span class="dt">oos =</span> oos)</span>
<span id="cb11-33"><a href="#cb11-33"></a>    approx_elpds_1sap[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">log_mean_exp</span>(loglik[, oos])</span>
<span id="cb11-34"><a href="#cb11-34"></a>  } <span class="cf">else</span> {</span>
<span id="cb11-35"><a href="#cb11-35"></a>    lw &lt;-<span class="st"> </span><span class="kw">weights</span>(psis_obj, <span class="dt">normalize =</span> <span class="ot">TRUE</span>)[, <span class="dv">1</span>]</span>
<span id="cb11-36"><a href="#cb11-36"></a>    approx_elpds_1sap[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">log_sum_exp</span>(lw <span class="op">+</span><span class="st"> </span>loglik[, oos])</span>
<span id="cb11-37"><a href="#cb11-37"></a>  }</span>
<span id="cb11-38"><a href="#cb11-38"></a>} </span></code></pre></div>
<p>We see that the final Pareto-<span class="math inline">\(k\)</span>-estimates are mostly well below the threshold and that we only needed to refit the model a few times:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a>plot_ks &lt;-<span class="st"> </span><span class="cf">function</span>(ks, ids, <span class="dt">thres =</span> <span class="fl">0.6</span>) {</span>
<span id="cb12-2"><a href="#cb12-2"></a>  dat_ks &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">ks =</span> ks, <span class="dt">ids =</span> ids)</span>
<span id="cb12-3"><a href="#cb12-3"></a>  <span class="kw">ggplot</span>(dat_ks, <span class="kw">aes</span>(<span class="dt">x =</span> ids, <span class="dt">y =</span> ks)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> ks <span class="op">&gt;</span><span class="st"> </span>thres), <span class="dt">shape =</span> <span class="dv">3</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="st">    </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> thres, <span class="dt">linetype =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;red2&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="st">    </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;cornflowerblue&quot;</span>, <span class="st">&quot;darkblue&quot;</span>)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Data point&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Pareto k&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="st">    </span><span class="kw">ylim</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>)</span>
<span id="cb12-9"><a href="#cb12-9"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="kw">cat</span>(<span class="st">&quot;Using threshold &quot;</span>, k_thres, </span>
<span id="cb13-2"><a href="#cb13-2"></a>    <span class="st">&quot;, model was refit &quot;</span>, <span class="kw">length</span>(refits), </span>
<span id="cb13-3"><a href="#cb13-3"></a>    <span class="st">&quot; times, at observations&quot;</span>, refits)</span>
<span id="cb13-4"><a href="#cb13-4"></a></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="kw">plot_ks</span>(ks, (L <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span></code></pre></div>
<p>The approximate 1-SAP ELPD is remarkably similar to the exact 1-SAP ELPD computed above, which indicates our algorithm to compute approximate 1-SAP worked well for the present data and model.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a>approx_elpd_1sap &lt;-<span class="st"> </span><span class="kw">sum</span>(approx_elpds_1sap, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="kw">rbind_print</span>(</span>
<span id="cb14-3"><a href="#cb14-3"></a>  <span class="st">&quot;approx LFO&quot;</span> =<span class="st"> </span>approx_elpd_1sap,</span>
<span id="cb14-4"><a href="#cb14-4"></a>  <span class="st">&quot;exact LFO&quot;</span> =<span class="st"> </span>exact_elpd_1sap</span>
<span id="cb14-5"><a href="#cb14-5"></a>)</span></code></pre></div>
<p>Plotting exact against approximate predictions, we see that no approximation value deviates far from its exact counterpart, providing further evidence for the good quality of our approximation.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>dat_elpd &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb15-2"><a href="#cb15-2"></a>  <span class="dt">approx_elpd =</span> approx_elpds_1sap,</span>
<span id="cb15-3"><a href="#cb15-3"></a>  <span class="dt">exact_elpd =</span> exact_elpds_1sap</span>
<span id="cb15-4"><a href="#cb15-4"></a>)</span>
<span id="cb15-5"><a href="#cb15-5"></a></span>
<span id="cb15-6"><a href="#cb15-6"></a><span class="kw">ggplot</span>(dat_elpd, <span class="kw">aes</span>(<span class="dt">x =</span> approx_elpd, <span class="dt">y =</span> exact_elpd)) <span class="op">+</span></span>
<span id="cb15-7"><a href="#cb15-7"></a><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">color =</span> <span class="st">&quot;gray30&quot;</span>) <span class="op">+</span></span>
<span id="cb15-8"><a href="#cb15-8"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb15-9"><a href="#cb15-9"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Approximate ELPDs&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Exact ELPDs&quot;</span>)</span></code></pre></div>
<p>We can also look at the maximum difference and average difference between the approximate and exact ELPD calculations, which also indicate a ver close approximation:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a>max_diff &lt;-<span class="st"> </span><span class="kw">with</span>(dat_elpd, <span class="kw">max</span>(<span class="kw">abs</span>(approx_elpd <span class="op">-</span><span class="st"> </span>exact_elpd), <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))</span>
<span id="cb16-2"><a href="#cb16-2"></a>mean_diff &lt;-<span class="st"> </span><span class="kw">with</span>(dat_elpd, <span class="kw">mean</span>(<span class="kw">abs</span>(approx_elpd <span class="op">-</span><span class="st"> </span>exact_elpd), <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))</span>
<span id="cb16-3"><a href="#cb16-3"></a></span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="kw">rbind_print</span>(</span>
<span id="cb16-5"><a href="#cb16-5"></a>  <span class="st">&quot;Max diff&quot;</span> =<span class="st"> </span><span class="kw">round</span>(max_diff, <span class="dv">2</span>), </span>
<span id="cb16-6"><a href="#cb16-6"></a>  <span class="st">&quot;Mean diff&quot;</span> =<span class="st">  </span><span class="kw">round</span>(mean_diff, <span class="dv">3</span>)</span>
<span id="cb16-7"><a href="#cb16-7"></a>)</span></code></pre></div>
</div>
</div>
<div id="m-step-ahead-predictions-leaving-out-all-future-values" class="section level2">
<h2><span class="math inline">\(M\)</span>-step-ahead predictions leaving out all future values</h2>
<p>To illustrate the application of <span class="math inline">\(M\)</span>-SAP for <span class="math inline">\(M &gt; 1\)</span>, we next compute exact and approximate LFO-CV for the 4-SAP case.</p>
<div id="exact-m-step-ahead-predictions" class="section level3">
<h3>Exact <span class="math inline">\(M\)</span>-step-ahead predictions</h3>
<p>The necessary steps are the same as for 1-SAP with the exception that the log-density values of interest are now the sums of the log predictive densities of four consecutive observations. Further, the stability of the PSIS approximation actually stays the same for all <span class="math inline">\(M\)</span> as it only depends on the number of observations we leave out, not on the number of observations we predict.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a>M &lt;-<span class="st"> </span><span class="dv">4</span></span>
<span id="cb17-2"><a href="#cb17-2"></a>loglikm &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> <span class="kw">nsamples</span>(fit), <span class="dt">ncol =</span> N)</span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="cf">for</span> (i <span class="cf">in</span> L<span class="op">:</span>(N <span class="op">-</span><span class="st"> </span>M)) {</span>
<span id="cb17-4"><a href="#cb17-4"></a>  past &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>i</span>
<span id="cb17-5"><a href="#cb17-5"></a>  oos &lt;-<span class="st"> </span>(i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>(i <span class="op">+</span><span class="st"> </span>M)</span>
<span id="cb17-6"><a href="#cb17-6"></a>  df_past &lt;-<span class="st"> </span>df[past, , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb17-7"><a href="#cb17-7"></a>  df_oos &lt;-<span class="st"> </span>df[<span class="kw">c</span>(past, oos), , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb17-8"><a href="#cb17-8"></a>  fit_past &lt;-<span class="st"> </span><span class="kw">update</span>(fit, <span class="dt">newdata =</span> df_past, <span class="dt">recompile =</span> <span class="ot">FALSE</span>)</span>
<span id="cb17-9"><a href="#cb17-9"></a>  loglik &lt;-<span class="st"> </span><span class="kw">log_lik</span>(fit_past, <span class="dt">newdata =</span> df_oos, <span class="dt">oos =</span> oos)</span>
<span id="cb17-10"><a href="#cb17-10"></a>  loglikm[, i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rowSums</span>(loglik[, oos])</span>
<span id="cb17-11"><a href="#cb17-11"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a>exact_elpds_4sap &lt;-<span class="st"> </span><span class="kw">apply</span>(loglikm, <span class="dv">2</span>, log_mean_exp)</span>
<span id="cb18-2"><a href="#cb18-2"></a>(exact_elpd_4sap &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">ELPD =</span> <span class="kw">sum</span>(exact_elpds_4sap, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)))</span></code></pre></div>
</div>
<div id="approximate-m-step-ahead-predictions" class="section level3">
<h3>Approximate <span class="math inline">\(M\)</span>-step-ahead predictions</h3>
<p>Computing the approximate PSIS-LFO-CV for the 4-SAP case is a little bit more involved than the approximate version for the 1-SAP case, although the underlying principles remain the same.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a>approx_elpds_4sap &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, N)</span>
<span id="cb19-2"><a href="#cb19-2"></a></span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="co"># initialize the process for i = L</span></span>
<span id="cb19-4"><a href="#cb19-4"></a>past &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>L</span>
<span id="cb19-5"><a href="#cb19-5"></a>oos &lt;-<span class="st"> </span>(L <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>(L <span class="op">+</span><span class="st"> </span>M)</span>
<span id="cb19-6"><a href="#cb19-6"></a>df_past &lt;-<span class="st"> </span>df[past, , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb19-7"><a href="#cb19-7"></a>df_oos &lt;-<span class="st"> </span>df[<span class="kw">c</span>(past, oos), , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb19-8"><a href="#cb19-8"></a>fit_past &lt;-<span class="st"> </span><span class="kw">update</span>(fit, <span class="dt">newdata =</span> df_past, <span class="dt">recompile =</span> <span class="ot">FALSE</span>)</span>
<span id="cb19-9"><a href="#cb19-9"></a>loglik &lt;-<span class="st"> </span><span class="kw">log_lik</span>(fit_past, <span class="dt">newdata =</span> df_oos, <span class="dt">oos =</span> oos)</span>
<span id="cb19-10"><a href="#cb19-10"></a>loglikm &lt;-<span class="st"> </span><span class="kw">rowSums</span>(loglik[, oos])</span>
<span id="cb19-11"><a href="#cb19-11"></a>approx_elpds_1sap[L <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">log_mean_exp</span>(loglikm)</span>
<span id="cb19-12"><a href="#cb19-12"></a></span>
<span id="cb19-13"><a href="#cb19-13"></a><span class="co"># iterate over i &gt; L</span></span>
<span id="cb19-14"><a href="#cb19-14"></a>i_refit &lt;-<span class="st"> </span>L</span>
<span id="cb19-15"><a href="#cb19-15"></a>refits &lt;-<span class="st"> </span>L</span>
<span id="cb19-16"><a href="#cb19-16"></a>ks &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb19-17"><a href="#cb19-17"></a><span class="cf">for</span> (i <span class="cf">in</span> (L <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>(N <span class="op">-</span><span class="st"> </span>M)) {</span>
<span id="cb19-18"><a href="#cb19-18"></a>  past &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>i</span>
<span id="cb19-19"><a href="#cb19-19"></a>  oos &lt;-<span class="st"> </span>(i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>(i <span class="op">+</span><span class="st"> </span>M)</span>
<span id="cb19-20"><a href="#cb19-20"></a>  df_past &lt;-<span class="st"> </span>df[past, , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb19-21"><a href="#cb19-21"></a>  df_oos &lt;-<span class="st"> </span>df[<span class="kw">c</span>(past, oos), , drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb19-22"><a href="#cb19-22"></a>  loglik &lt;-<span class="st"> </span><span class="kw">log_lik</span>(fit_past, <span class="dt">newdata =</span> df_oos, <span class="dt">oos =</span> oos)</span>
<span id="cb19-23"><a href="#cb19-23"></a>  </span>
<span id="cb19-24"><a href="#cb19-24"></a>  logratio &lt;-<span class="st"> </span><span class="kw">sum_log_ratios</span>(loglik, (i_refit <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>i)</span>
<span id="cb19-25"><a href="#cb19-25"></a>  psis_obj &lt;-<span class="st"> </span><span class="kw">suppressWarnings</span>(<span class="kw">psis</span>(logratio))</span>
<span id="cb19-26"><a href="#cb19-26"></a>  k &lt;-<span class="st"> </span><span class="kw">pareto_k_values</span>(psis_obj)</span>
<span id="cb19-27"><a href="#cb19-27"></a>  ks &lt;-<span class="st"> </span><span class="kw">c</span>(ks, k)</span>
<span id="cb19-28"><a href="#cb19-28"></a>  <span class="cf">if</span> (k <span class="op">&gt;</span><span class="st"> </span>k_thres) {</span>
<span id="cb19-29"><a href="#cb19-29"></a>    <span class="co"># refit the model based on the first i observations</span></span>
<span id="cb19-30"><a href="#cb19-30"></a>    i_refit &lt;-<span class="st"> </span>i</span>
<span id="cb19-31"><a href="#cb19-31"></a>    refits &lt;-<span class="st"> </span><span class="kw">c</span>(refits, i)</span>
<span id="cb19-32"><a href="#cb19-32"></a>    fit_past &lt;-<span class="st"> </span><span class="kw">update</span>(fit_past, <span class="dt">newdata =</span> df_past, <span class="dt">recompile =</span> <span class="ot">FALSE</span>)</span>
<span id="cb19-33"><a href="#cb19-33"></a>    loglik &lt;-<span class="st"> </span><span class="kw">log_lik</span>(fit_past, <span class="dt">newdata =</span> df_oos, <span class="dt">oos =</span> oos)</span>
<span id="cb19-34"><a href="#cb19-34"></a>    loglikm &lt;-<span class="st"> </span><span class="kw">rowSums</span>(loglik[, oos])</span>
<span id="cb19-35"><a href="#cb19-35"></a>    approx_elpds_4sap[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">log_mean_exp</span>(loglikm)</span>
<span id="cb19-36"><a href="#cb19-36"></a>  } <span class="cf">else</span> {</span>
<span id="cb19-37"><a href="#cb19-37"></a>    lw &lt;-<span class="st"> </span><span class="kw">weights</span>(psis_obj, <span class="dt">normalize =</span> <span class="ot">TRUE</span>)[, <span class="dv">1</span>]</span>
<span id="cb19-38"><a href="#cb19-38"></a>    loglikm &lt;-<span class="st"> </span><span class="kw">rowSums</span>(loglik[, oos])</span>
<span id="cb19-39"><a href="#cb19-39"></a>    approx_elpds_4sap[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">log_sum_exp</span>(lw <span class="op">+</span><span class="st"> </span>loglikm)</span>
<span id="cb19-40"><a href="#cb19-40"></a>  }</span>
<span id="cb19-41"><a href="#cb19-41"></a>} </span></code></pre></div>
<p>Again, we see that the final Pareto-<span class="math inline">\(k\)</span>-estimates are mostly well below the threshold and that we only needed to refit the model a few times:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="kw">cat</span>(<span class="st">&quot;Using threshold &quot;</span>, k_thres, </span>
<span id="cb20-2"><a href="#cb20-2"></a>    <span class="st">&quot;, model was refit &quot;</span>, <span class="kw">length</span>(refits), </span>
<span id="cb20-3"><a href="#cb20-3"></a>    <span class="st">&quot; times, at observations&quot;</span>, refits)</span>
<span id="cb20-4"><a href="#cb20-4"></a></span>
<span id="cb20-5"><a href="#cb20-5"></a><span class="kw">plot_ks</span>(ks, (L <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>(N <span class="op">-</span><span class="st"> </span>M))</span></code></pre></div>
<p>The approximate ELPD computed for the 4-SAP case is not as close to its exact counterpart as in the 1-SAP case. In general, the larger <span class="math inline">\(M\)</span>, the larger the variation of the approximate ELPD around the exact ELPD. It turns out that the ELPD estimates of AR-models with <span class="math inline">\(M&gt;1\)</span> show particular variation due to their predictions’ dependency on other predicted values. In Bürkner et al. (2020) we provide further explanation and simulations for these cases.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a>approx_elpd_4sap &lt;-<span class="st"> </span><span class="kw">sum</span>(approx_elpds_4sap, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="kw">rbind_print</span>(</span>
<span id="cb21-3"><a href="#cb21-3"></a>  <span class="st">&quot;Approx LFO&quot;</span> =<span class="st"> </span>approx_elpd_4sap,</span>
<span id="cb21-4"><a href="#cb21-4"></a>  <span class="st">&quot;Exact LFO&quot;</span> =<span class="st"> </span>exact_elpd_4sap</span>
<span id="cb21-5"><a href="#cb21-5"></a>)</span></code></pre></div>
<p>Plotting exact against approximate pointwise predictions confirms that, for a few specific data points, the approximate predictions underestimate the exact predictions.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a>dat_elpd_4sap &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb22-2"><a href="#cb22-2"></a>  <span class="dt">approx_elpd =</span> approx_elpds_4sap,</span>
<span id="cb22-3"><a href="#cb22-3"></a>  <span class="dt">exact_elpd =</span> exact_elpds_4sap</span>
<span id="cb22-4"><a href="#cb22-4"></a>)</span>
<span id="cb22-5"><a href="#cb22-5"></a></span>
<span id="cb22-6"><a href="#cb22-6"></a><span class="kw">ggplot</span>(dat_elpd_4sap, <span class="kw">aes</span>(<span class="dt">x =</span> approx_elpd, <span class="dt">y =</span> exact_elpd)) <span class="op">+</span></span>
<span id="cb22-7"><a href="#cb22-7"></a><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">color =</span> <span class="st">&quot;gray30&quot;</span>) <span class="op">+</span></span>
<span id="cb22-8"><a href="#cb22-8"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb22-9"><a href="#cb22-9"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Approximate ELPDs&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Exact ELPDs&quot;</span>)</span></code></pre></div>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In this case study we have shown how to do carry out exact and approximate leave-future-out cross-validation for <span class="math inline">\(M\)</span>-step-ahead prediction tasks. For the data and model used in our example, the PSIS-LFO-CV algorithm provides reasonably stable and accurate results despite not requiring us to refit the model nearly as many times. For more details on approximate LFO-CV, we refer to Bürkner et al. (2020).</p>
<p><br /></p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Bürkner P. C., Gabry J., &amp; Vehtari A. (2020). Approximate leave-future-out cross-validation for Bayesian time series models. <em>Journal of Statistical Computation and Simulation</em>. :10.1080/00949655.2020.1783262. <a href="https://www.tandfonline.com/doi/full/10.1080/00949655.2020.1783262">Online</a>. <a href="https://arxiv.org/abs/1902.06281">arXiv preprint</a>.</p>
<p>Vehtari A., Gelman A., &amp; Gabry J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. <em>Statistics and Computing</em>, 27(5), 1413–1432. :10.1007/s11222-016-9696-4. <a href="http://link.springer.com/article/10.1007/s11222-016-9696-4">Online</a>. <a href="https://arxiv.org/abs/1507.04544">arXiv preprint arXiv:1507.04544</a>.</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2019). Pareto smoothed importance sampling. <a href="https://arxiv.org/abs/1507.02646">arXiv preprint arXiv:1507.02646</a>.</p>
<p><br /></p>
</div>
<div id="appendix" class="section level2">
<h2>Appendix</h2>
<div id="appendix-session-information" class="section level3">
<h3>Appendix: Session information</h3>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="kw">sessionInfo</span>()</span></code></pre></div>
</div>
<div id="appendix-licenses" class="section level3">
<h3>Appendix: Licenses</h3>
<ul>
<li>Code © 2018, Paul Bürkner, Jonah Gabry, Aki Vehtari (licensed under BSD-3).</li>
<li>Text © 2018, Paul Bürkner, Jonah Gabry, Aki Vehtari (licensed under CC-BY-NC 4.0).</li>
</ul>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
